{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f29516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import traceback\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Generator\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.callbacks import (\n",
    "  ModelCheckpoint,\n",
    "  EarlyStopping,\n",
    "  CSVLogger,\n",
    "  ReduceLROnPlateau,\n",
    "  TensorBoard,\n",
    ")\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from scipy.ndimage import distance_transform_edt, gaussian_filter\n",
    "\n",
    "import neurite as ne\n",
    "from docx import Document\n",
    "from docx.oxml.ns import qn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_DIR = \"./data\"\n",
    "\n",
    "BASE_DATA_PATH = LOCAL_DATA_DIR\n",
    "MODELS_BASE_PATH = os.path.join(LOCAL_DATA_DIR, 'Models')\n",
    "\n",
    "# Local-specific path structure\n",
    "ACDC_BASE = ''\n",
    "SUNNYBROOK_BASE = ''\n",
    "train_data = os.path.join(LOCAL_DATA_DIR, 'train')\n",
    "val_data = os.path.join(LOCAL_DATA_DIR, 'val')\n",
    "test_data = os.path.join(LOCAL_DATA_DIR, 'test')\n",
    "mask_data = os.path.join(LOCAL_DATA_DIR, 'ACDC-Masks-1')\n",
    "MODEL_TESTING_PATH = os.path.join(LOCAL_DATA_DIR, 'model_testing')\n",
    "\n",
    "train_simulated_data = os.path.join(LOCAL_DATA_DIR, 'Simulated_train')\n",
    "val_simulated_data = os.path.join(LOCAL_DATA_DIR, 'Simulated_val')\n",
    "test_simulated_data = os.path.join(LOCAL_DATA_DIR, 'Simulated_test')\n",
    "mask_simulated_data = os.path.join(LOCAL_DATA_DIR, 'Simulated_masks')\n",
    "displacement_simulated_data = os.path.join(LOCAL_DATA_DIR, 'Simulated_displacements')\n",
    "\n",
    "# Simulated data paths (already updated in your script)\n",
    "SIMULATED_DATA_PATH = test_simulated_data  # ./data/Simulated_test\n",
    "SIMULATED_MASK_PATH = mask_simulated_data  # ./data/Simulated_masks\n",
    "SIMULATED_DISP_PATH = displacement_simulated_data  # ./data/Simulated_displacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_paths(paths):\n",
    "    \"\"\"Verify existence of required paths with enhanced feedback\"\"\"\n",
    "    missing_paths = []\n",
    "    existing_paths = []\n",
    "\n",
    "    print(\"\\nChecking data paths:\")\n",
    "    for name, path in paths.items():\n",
    "        exists = os.path.exists(path)\n",
    "        status = \"✓\" if exists else \"✗\"\n",
    "        print(f\"  {status} {name}: {path}\")\n",
    "\n",
    "        if exists:\n",
    "            existing_paths.append(path)\n",
    "        else:\n",
    "            missing_paths.append(path)\n",
    "\n",
    "    return existing_paths, missing_paths\n",
    "\n",
    "# Check all critical paths\n",
    "paths_to_check = {\n",
    "    'Simulated Training': train_simulated_data,\n",
    "    'Simulated Validation': val_simulated_data,\n",
    "    'Simulated Testing': test_simulated_data,\n",
    "    'Simulated Masks': mask_simulated_data,\n",
    "    'Simulated Displacements': displacement_simulated_data,\n",
    "    'train data': train_data,\n",
    "    'val data': val_data,\n",
    "    'test data': test_data,\n",
    "    'mask data': mask_data, \n",
    "}\n",
    "\n",
    "existing, missing = check_paths(paths_to_check)\n",
    "\n",
    "if missing:\n",
    "    print(\"\\n⚠️ Missing paths detected!\")\n",
    "    print(f\"Please ensure your local data directory ({LOCAL_DATA_DIR}) contains:\")\n",
    "    print(\"- Simulated Training/Validation/Testing folders\")\n",
    "    print(\"- Simulated Masks folder\")\n",
    "    print(\"- Simulated Displacements folder\")\n",
    "    print(\"- ACDC-Masks-1 folder\")\n",
    "    print(\"- model_testing\")\n",
    "    print(\"- train/val/test folders\")\n",
    "    raise FileNotFoundError(\"Missing required data paths\")  # Uncomment to enforce strict checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### New BG to myocarduim mask weighting\n",
    "def create_weighted_mask_bg(mask, dilation_extent=5, sigma=2, myocardium_weight_factor=1):\n",
    "    \"\"\"\n",
    "    Generate a weighted mask with:\n",
    "    - Myocardium (label 1) = weight 2.0\n",
    "    - Smoothly decaying weights outward from the myocardium (controlled by `dilation_extent`).\n",
    "    - Multiplied by the ratio of background pixels to myocardium pixels for increased myocardium influence.\n",
    "\n",
    "    Args:\n",
    "        mask (np.ndarray): Input mask with labels {0, 1, 2}.\n",
    "        dilation_extent (int): Number of dilation iterations (higher = wider decay).\n",
    "        sigma (float): Smoothness of the decay (Gaussian blur).\n",
    "        myocardium_weight_factor (float): Additional factor to control the myocardium weight (default = 1).\n",
    "\n",
    "    Returns:\n",
    "        weighted_mask (np.ndarray): Weighted mask with values ≥1.0.\n",
    "    \"\"\"\n",
    "    # Ensure mask is 2D by squeezing singleton dimensions\n",
    "    mask = mask.squeeze()\n",
    "\n",
    "    # Extract myocardium (label 1) and background (label 0)\n",
    "    myocardium = (mask == 1).astype(np.float32)\n",
    "    background = (mask == 0).astype(np.float32)\n",
    "\n",
    "    # Compute the ratio of background pixels to myocardium pixels\n",
    "    num_background_pixels = np.sum(background)\n",
    "    num_myocardium_pixels = np.sum(myocardium)\n",
    "    ratio = num_background_pixels / (num_myocardium_pixels + 1e-6)  # Add epsilon to avoid division by zero\n",
    "\n",
    "    # Initialize dilated mask and process mask\n",
    "    dilated_mask = myocardium.copy()\n",
    "    process_mask = myocardium.copy()\n",
    "\n",
    "    # Kernel for dilation (7x7 ellipse)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
    "\n",
    "    # Gradually reduce the added weight per iteration\n",
    "    initial_value = 0.9\n",
    "    step_size = initial_value / dilation_extent  # Controls decay per iteration\n",
    "\n",
    "    for i in range(dilation_extent):\n",
    "        old_process_mask = process_mask.copy()\n",
    "        process_mask = cv2.dilate(process_mask, kernel)\n",
    "\n",
    "        # Identify newly added pixels (boundary of the dilated region)\n",
    "        added_region = (process_mask - old_process_mask).astype(np.float32)\n",
    "\n",
    "        # Ensure added_region has the same number of dimensions as dilated_mask\n",
    "        added_region = added_region[..., np.newaxis] if added_region.ndim < dilated_mask.ndim else added_region\n",
    "\n",
    "        # Compute weight for this iteration (decays linearly with iterations)\n",
    "        current_weight = initial_value - i * step_size\n",
    "\n",
    "        # Update dilated mask with decaying weights\n",
    "        dilated_mask += added_region * current_weight\n",
    "\n",
    "    # Smooth the dilation\n",
    "    dilated_mask[myocardium.astype(bool)] = 1.0\n",
    "    smoothed_mask = gaussian_filter(dilated_mask, sigma=sigma)\n",
    "    # smoothed_mask = (1 - np.exp(1.2 * dilated_mask)) / (1 - np.exp(1.2))\n",
    "\n",
    "    # Multiply by the ratio of background to myocardium pixels\n",
    "    smoothed_mask *= ratio\n",
    "\n",
    "    # Apply the myocardium weight factor\n",
    "    smoothed_mask *= myocardium_weight_factor\n",
    "\n",
    "    # Add 1 to the mask\n",
    "    smoothed_mask += 1.0\n",
    "\n",
    "    mask_sum = tf.reduce_sum(smoothed_mask)  # Scalar\n",
    "\n",
    "    # Compute the number of pixels in the mask\n",
    "    num_pixels = tf.reduce_sum(tf.ones_like(smoothed_mask))  # Scalar\n",
    "\n",
    "    # Compute the normalization factor\n",
    "    normalization_factor = num_pixels / (mask_sum + 1e-6)\n",
    "\n",
    "    smoothed_mask = smoothed_mask * normalization_factor\n",
    "\n",
    "    return smoothed_mask[..., np.newaxis]\n",
    "###### New Inverted mask for smoothing\n",
    "def create_weighted_mask_inverted(mask, dilation_extent=5, sigma=2):\n",
    "    \"\"\"\n",
    "    Generate a weighted mask with INVERTED weights (background prioritized):\n",
    "    - Background (label 0) = weight 2.0\n",
    "    - Smoothly decaying weights inward from the background (controlled by `dilation_extent`).\n",
    "    - Same calculations as original, but with (1 - mask) applied before +1.0.\n",
    "\n",
    "    Args:\n",
    "        mask (np.ndarray): Input mask with labels {0, 1, 2}.\n",
    "        dilation_extent (int): Number of dilation iterations (higher = wider decay).\n",
    "        sigma (float): Smoothness of the decay (Gaussian blur).\n",
    "\n",
    "    Returns:\n",
    "        weighted_mask (np.ndarray): Weighted mask with values ≥1.0.\n",
    "    \"\"\"\n",
    "    # Ensure mask is 2D by squeezing singleton dimensions\n",
    "    mask = mask.squeeze()\n",
    "\n",
    "    # Extract myocardium (label 1) - we'll still dilate from myocardium as in original\n",
    "    myocardium = (mask == 1).astype(np.float32)\n",
    "\n",
    "    # Initialize dilated mask and process mask (same as original)\n",
    "    dilated_mask = myocardium.copy()\n",
    "    process_mask = myocardium.copy()\n",
    "\n",
    "    # Kernel for dilation (7x7 ellipse)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
    "\n",
    "    # Gradually reduce the added weight per iteration (same as original)\n",
    "    initial_value = 0.9\n",
    "    step_size = initial_value / dilation_extent\n",
    "\n",
    "    for i in range(dilation_extent):\n",
    "        old_process_mask = process_mask.copy()\n",
    "        process_mask = cv2.dilate(process_mask, kernel)\n",
    "\n",
    "        # Identify newly added pixels (boundary of the dilated region)\n",
    "        added_region = (process_mask - old_process_mask).astype(np.float32)\n",
    "        added_region = added_region[..., np.newaxis] if added_region.ndim < dilated_mask.ndim else added_region\n",
    "\n",
    "        # Compute weight for this iteration (decays linearly with iterations)\n",
    "        current_weight = initial_value - i * step_size\n",
    "\n",
    "        # Update dilated mask with decaying weights\n",
    "        dilated_mask += added_region * current_weight\n",
    "\n",
    "    # Smooth the dilation\n",
    "    dilated_mask[myocardium.astype(bool)] = 1.0\n",
    "    smoothed_mask = gaussian_filter(dilated_mask, sigma=sigma)\n",
    "    # smoothed_mask = (1 - np.exp(1.2 * dilated_mask)) / (1 - np.exp(1.2))\n",
    "\n",
    "    ##### KEY MODIFICATION: Invert weights here (before +1.0) #####\n",
    "    smoothed_mask = 1.0 - smoothed_mask\n",
    "\n",
    "    # Add 1 to the mask (now background will be ~2.0, myocardium ~1.0)\n",
    "    smoothed_mask += 1.0\n",
    "\n",
    "    # Normalization (same as original, but using smoothed_mask)\n",
    "    mask_sum = tf.reduce_sum(smoothed_mask)  # Scalar\n",
    "    num_pixels = tf.reduce_sum(tf.ones_like(smoothed_mask))  # Scalar\n",
    "    normalization_factor = num_pixels / (mask_sum + 1e-6)\n",
    "\n",
    "    return smoothed_mask[..., np.newaxis] * normalization_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "@dataclass\n",
    "class DataSample:\n",
    "    \"\"\"Simple container for a data sample\"\"\"\n",
    "    moving: np.ndarray\n",
    "    fixed: np.ndarray\n",
    "    masks: Optional[Tuple[np.ndarray, np.ndarray]] = None\n",
    "    patient_id: str = \"\"\n",
    "    source: str = \"real\"\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Data preprocessor that collects and saves training data to HDF5 files.\n",
    "    Replicates the logic from SimpleHybridDataLoader.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        real_data_paths: Dict[str, List[str]],\n",
    "        simulated_data_paths: Optional[Dict[str, List[str]]] = None,\n",
    "        mask_root_path: Optional[str] = None,\n",
    "        simulated_mask_root_path: Optional[str] = None,\n",
    "        output_dir: str = \"Training_data\",\n",
    "        max_frame_skip: int = 7,\n",
    "        min_frame_skip: int = 3,\n",
    "        shuffle: bool = True,\n",
    "        seed: int = 42,\n",
    "        use_mask: bool = True,\n",
    "        chunk_size: int = 1000  # Process data in chunks to manage memory\n",
    "    ):\n",
    "        self.real_data_paths = real_data_paths\n",
    "        self.simulated_data_paths = simulated_data_paths or {'train': [], 'val': [], 'test': []}\n",
    "        self.mask_root = mask_root_path\n",
    "        self.simulated_mask_root = simulated_mask_root_path\n",
    "        self.output_dir = output_dir\n",
    "        self.max_frame_skip = max_frame_skip\n",
    "        self.min_frame_skip = min_frame_skip\n",
    "        self.shuffle = shuffle\n",
    "        self.use_mask = use_mask\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        if self.use_mask and (not self.mask_root or not self.simulated_mask_root):\n",
    "            raise ValueError(\"Mask paths required when use_mask=True\")\n",
    "        \n",
    "        logging.info(f\"DataPreprocessor initialized. Output directory: {self.output_dir}\")\n",
    "        logging.info(f\"Using masks: {self.use_mask}\")\n",
    "\n",
    "    def process_and_save_all_splits(self):\n",
    "        \"\"\"Process and save all data splits\"\"\"\n",
    "        splits = ['train', 'val', 'test']\n",
    "        \n",
    "        for split in splits:\n",
    "            logging.info(f\"Processing {split} split...\")\n",
    "            try:\n",
    "                self.process_and_save_split(split)\n",
    "                logging.info(f\"Successfully processed and saved {split} split\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process {split} split: {e}\")\n",
    "                raise\n",
    "\n",
    "    def process_and_save_split(self, split: str):\n",
    "        \"\"\"Process and save a single data split\"\"\"\n",
    "        logging.info(f\"Collecting samples for {split} split...\")\n",
    "        \n",
    "        # Collect all samples first\n",
    "        raw_samples = []\n",
    "        \n",
    "        # Load real data\n",
    "        for i, data_path in enumerate(tqdm(self.real_data_paths.get(split, []), desc=f\"Collecting real samples for {split}\")):\n",
    "            self._collect_real_samples(data_path, raw_samples)\n",
    "        \n",
    "        logging.info(f\"Collected {len(raw_samples)} real raw samples for {split} split\")\n",
    "        \n",
    "        # Load simulated data\n",
    "        for i, data_path in enumerate(tqdm(self.simulated_data_paths.get(split, []), desc=f\"Collecting simulated samples for {split}\")):\n",
    "            self._collect_simulated_samples(data_path, raw_samples)\n",
    "        \n",
    "        logging.info(f\"Collected {len(raw_samples)} total raw samples for {split} split\")\n",
    "\n",
    "        if not raw_samples:\n",
    "            logging.warning(f\"No samples found for {split} split\")\n",
    "            # Create empty arrays\n",
    "            self._save_empty_split(split)\n",
    "            return\n",
    "        \n",
    "        if self.shuffle:\n",
    "            random.shuffle(raw_samples)\n",
    "        \n",
    "        # Process and save in chunks to manage memory\n",
    "        self._process_and_save_in_chunks(raw_samples, split)\n",
    "\n",
    "    def _process_and_save_in_chunks(self, raw_samples: List[DataSample], split: str):\n",
    "        \"\"\"Process samples in chunks and save to HDF5\"\"\"\n",
    "        total_samples = len(raw_samples)\n",
    "        valid_samples_count = 0\n",
    "        \n",
    "        # Initialize lists to collect all processed data\n",
    "        all_moving = []\n",
    "        all_fixed = []\n",
    "        all_zero_phi = []\n",
    "        \n",
    "        logging.info(f\"Processing {total_samples} samples for {split} split in chunks of {self.chunk_size}\")\n",
    "        \n",
    "        # Process in chunks\n",
    "        for chunk_start in range(0, total_samples, self.chunk_size):\n",
    "            chunk_end = min(chunk_start + self.chunk_size, total_samples)\n",
    "            chunk_samples = raw_samples[chunk_start:chunk_end]\n",
    "            \n",
    "            logging.info(f\"Processing chunk {chunk_start//self.chunk_size + 1}/{(total_samples-1)//self.chunk_size + 1} \"\n",
    "                        f\"(samples {chunk_start+1}-{chunk_end})\")\n",
    "            \n",
    "            # Process chunk\n",
    "            chunk_moving = []\n",
    "            chunk_fixed = []\n",
    "            chunk_zero_phi = []\n",
    "            \n",
    "            for i, sample in enumerate(tqdm(chunk_samples, desc=f\"Processing {split} chunk\")):\n",
    "                try:\n",
    "                    processed_moving, processed_fixed, processed_zero_phi = self._preprocess_sample(sample)\n",
    "                    \n",
    "                    if processed_moving is not None:\n",
    "                        chunk_moving.append(processed_moving)\n",
    "                        chunk_fixed.append(processed_fixed)\n",
    "                        chunk_zero_phi.append(processed_zero_phi)\n",
    "                        valid_samples_count += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Failed to preprocess sample {sample.patient_id}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Add chunk data to overall lists\n",
    "            if chunk_moving:\n",
    "                all_moving.extend(chunk_moving)\n",
    "                all_fixed.extend(chunk_fixed)\n",
    "                all_zero_phi.extend(chunk_zero_phi)\n",
    "            \n",
    "            # Clear chunk data to free memory\n",
    "            del chunk_moving, chunk_fixed, chunk_zero_phi\n",
    "            \n",
    "            logging.info(f\"Chunk processed. Valid samples so far: {valid_samples_count}\")\n",
    "        \n",
    "        if not all_moving:\n",
    "            logging.error(f\"No valid samples after preprocessing for {split} split\")\n",
    "            self._save_empty_split(split)\n",
    "            return\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        logging.info(f\"Converting {len(all_moving)} samples to numpy arrays...\")\n",
    "        final_moving = np.stack(all_moving, axis=0)\n",
    "        final_fixed = np.stack(all_fixed, axis=0)\n",
    "        final_zero_phi = np.stack(all_zero_phi, axis=0)\n",
    "        \n",
    "        # Clear intermediate lists to free memory\n",
    "        del all_moving, all_fixed, all_zero_phi\n",
    "        \n",
    "        # Save to HDF5\n",
    "        self._save_to_hdf5(final_moving, final_fixed, final_zero_phi, split)\n",
    "        \n",
    "        logging.info(f\"Successfully processed and saved {valid_samples_count} valid samples for {split}\")\n",
    "        logging.info(f\"{split} shapes - Moving: {final_moving.shape}, \"\n",
    "                    f\"Fixed: {final_fixed.shape}, \"\n",
    "                    f\"Zero_phi: {final_zero_phi.shape}\")\n",
    "\n",
    "    def _save_to_hdf5(self, moving: np.ndarray, fixed: np.ndarray, zero_phi: np.ndarray, split: str):\n",
    "        \"\"\"Save processed data to HDF5 file\"\"\"\n",
    "        output_path = os.path.join(self.output_dir, f\"{split}_data.h5\")\n",
    "        \n",
    "        logging.info(f\"Saving {split} data to {output_path}\")\n",
    "        \n",
    "        with h5py.File(output_path, 'w') as f:\n",
    "            # Save arrays with compression\n",
    "            f.create_dataset('moving', data=moving, compression='gzip', compression_opts=9)\n",
    "            f.create_dataset('fixed', data=fixed, compression='gzip', compression_opts=9)\n",
    "            f.create_dataset('zero_phi', data=zero_phi, compression='gzip', compression_opts=9)\n",
    "            \n",
    "            # Save metadata\n",
    "            f.attrs['num_samples'] = moving.shape[0]\n",
    "            f.attrs['use_mask'] = self.use_mask\n",
    "            f.attrs['moving_shape'] = str(moving.shape)\n",
    "            f.attrs['fixed_shape'] = str(fixed.shape)\n",
    "            f.attrs['zero_phi_shape'] = str(zero_phi.shape)\n",
    "        \n",
    "        logging.info(f\"Successfully saved {split} data to {output_path}\")\n",
    "\n",
    "    def _save_empty_split(self, split: str):\n",
    "        \"\"\"Save empty arrays for splits with no data\"\"\"\n",
    "        output_path = os.path.join(self.output_dir, f\"{split}_data.h5\")\n",
    "        \n",
    "        fixed_channels = 3 if self.use_mask else 1\n",
    "        \n",
    "        empty_moving = np.empty((0, 128, 128, 1), dtype=np.float32)\n",
    "        empty_fixed = np.empty((0, 128, 128, fixed_channels), dtype=np.float32)\n",
    "        empty_zero_phi = np.empty((0, 128, 128, 2), dtype=np.float32)\n",
    "        \n",
    "        with h5py.File(output_path, 'w') as f:\n",
    "            f.create_dataset('moving', data=empty_moving)\n",
    "            f.create_dataset('fixed', data=empty_fixed)\n",
    "            f.create_dataset('zero_phi', data=empty_zero_phi)\n",
    "            \n",
    "            f.attrs['num_samples'] = 0\n",
    "            f.attrs['use_mask'] = self.use_mask\n",
    "            f.attrs['moving_shape'] = str(empty_moving.shape)\n",
    "            f.attrs['fixed_shape'] = str(empty_fixed.shape)\n",
    "            f.attrs['zero_phi_shape'] = str(empty_zero_phi.shape)\n",
    "        \n",
    "        logging.warning(f\"Saved empty {split} data to {output_path}\")\n",
    "\n",
    "    def _preprocess_sample(self, sample: DataSample) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "        \"\"\"Preprocess a single sample into final format\"\"\"\n",
    "        try:\n",
    "            # Validate sample first\n",
    "            if not self._validate_sample_shapes(sample) or not self._validate_sample_data(sample):\n",
    "                return None, None, None\n",
    "            \n",
    "            # Process moving image (always same)\n",
    "            processed_moving = sample.moving.copy()  # (128, 128, 1)\n",
    "            \n",
    "            # Process fixed image (with or without mask)\n",
    "            if self.use_mask and sample.masks:\n",
    "                mask_moving, mask_fixed = sample.masks\n",
    "                mask_fixed_weighted = create_weighted_mask_bg(mask_fixed)\n",
    "                mask_moving_weighted = create_weighted_mask_inverted(mask_moving)\n",
    "                \n",
    "                processed_fixed = np.concatenate([\n",
    "                    sample.fixed,           # (128, 128, 1)\n",
    "                    mask_fixed_weighted,    # (128, 128, 1)\n",
    "                    mask_moving_weighted    # (128, 128, 1)\n",
    "                ], axis=-1)  # (128, 128, 3)\n",
    "            else:\n",
    "                processed_fixed = sample.fixed.copy()  # (128, 128, 1)\n",
    "            \n",
    "            # Create zero_phi\n",
    "            processed_zero_phi = np.zeros((128, 128, 2), dtype=np.float32)\n",
    "            \n",
    "            # Set second channel of zero_phi if using masks\n",
    "            if self.use_mask and sample.masks:\n",
    "                processed_zero_phi[..., 1] = processed_fixed[..., 2]\n",
    "                # First channel remains zero\n",
    "            \n",
    "            return processed_moving, processed_fixed, processed_zero_phi\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error preprocessing sample {sample.patient_id}: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def _collect_real_samples(self, data_path: str, sample_list: List[DataSample]):\n",
    "        \"\"\"Collect real data samples\"\"\"\n",
    "        if not os.path.isdir(data_path):\n",
    "            logging.warning(f\"Directory not found: {data_path}\")\n",
    "            return\n",
    "        \n",
    "        patient_folders = [f for f in os.listdir(data_path) \n",
    "                          if f.startswith(\"patient\") and os.path.isdir(os.path.join(data_path, f))]\n",
    "        \n",
    "        logging.info(f\"Collecting real samples from {len(patient_folders)} patient folders in {data_path}\")\n",
    "        \n",
    "        for patient_folder in patient_folders:\n",
    "            patient_path = os.path.join(data_path, patient_folder)\n",
    "            pairs = self._extract_real_pairs(patient_path)\n",
    "            \n",
    "            for pair in pairs:\n",
    "                sample = self._load_real_sample(patient_path, pair, patient_folder)\n",
    "                if sample:\n",
    "                    sample_list.append(sample)\n",
    "\n",
    "    def _collect_simulated_samples(self, data_path: str, sample_list: List[DataSample]):\n",
    "        \"\"\"Collect simulated data samples\"\"\"\n",
    "        if not os.path.isdir(data_path):\n",
    "            logging.warning(f\"Directory not found: {data_path}\")\n",
    "            return\n",
    "        \n",
    "        patient_folders = [f for f in os.listdir(data_path) \n",
    "                          if f.startswith(\"patient\") and os.path.isdir(os.path.join(data_path, f))]\n",
    "        \n",
    "        logging.info(f\"Collecting simulated samples from {len(patient_folders)} patient folders in {data_path}\")\n",
    "        \n",
    "        for patient_folder in patient_folders:\n",
    "            patient_path = os.path.join(data_path, patient_folder)\n",
    "            pairs = self._extract_simulated_pairs(patient_path)\n",
    "            \n",
    "            for pair in pairs:\n",
    "                sample = self._load_simulated_sample(patient_path, pair, patient_folder)\n",
    "                if sample:\n",
    "                    sample_list.append(sample)\n",
    "\n",
    "    def _extract_real_pairs(self, patient_folder: str) -> List[Tuple]:\n",
    "        \"\"\"Extract valid frame pairs from real data\"\"\"\n",
    "        slice_times = {}\n",
    "        \n",
    "        for fname in os.listdir(patient_folder):\n",
    "            if not fname.endswith('.npy'):\n",
    "                continue\n",
    "            try:\n",
    "                parts = fname.split('_')\n",
    "                t_part = parts[1].lstrip('t')\n",
    "                z_part = parts[2].split('.')[0].lstrip('z')\n",
    "                t, z = int(t_part), int(z_part)\n",
    "                if z <= 1 or z >= 6:\n",
    "                    continue\n",
    "                slice_times.setdefault(z, []).append(t)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "        \n",
    "        valid_pairs = []\n",
    "        for z, times in slice_times.items():\n",
    "            sorted_times = sorted(times)\n",
    "            for i in range(len(sorted_times)):\n",
    "                current_t = sorted_times[i]\n",
    "                for j in range(i + 1, len(sorted_times)):\n",
    "                    next_t = sorted_times[j]\n",
    "                    frame_diff = next_t - current_t\n",
    "                    if self.min_frame_skip <= frame_diff <= self.max_frame_skip:\n",
    "                        valid_pairs.append((current_t, z, frame_diff, next_t, current_t))\n",
    "        \n",
    "        return valid_pairs\n",
    "\n",
    "    def _extract_simulated_pairs(self, patient_folder: str) -> List[Tuple]:\n",
    "        \"\"\"Extract valid frame pairs from simulated data\"\"\"\n",
    "        files = os.listdir(patient_folder)\n",
    "        slice_times = {}\n",
    "        actual_frames = set()\n",
    "        \n",
    "        for fname in files:\n",
    "            if not fname.endswith('.npy'):\n",
    "                continue\n",
    "            try:\n",
    "                base_part, frame_part = fname.rsplit('#', 1)\n",
    "                frame = frame_part.split('.')[0]\n",
    "                actual_frames.add(frame)\n",
    "                parts = base_part.split('_')\n",
    "                t_str, z_str = parts[-2].lstrip('t'), parts[-1].lstrip('z')\n",
    "                t, z = int(t_str), int(z_str)\n",
    "                slice_times.setdefault((z, t), []).append(frame)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "        \n",
    "        valid_pairs = []\n",
    "        for (z, t), frames in slice_times.items():\n",
    "            sorted_frames = sorted(frames, key=lambda x: int(x))\n",
    "            first_frame = next((frame for frame in sorted_frames if frame[-2:] == '_1'), None)\n",
    "            \n",
    "            for frame in sorted_frames:\n",
    "                if frame[-2:] != '_1' and first_frame in actual_frames and frame in actual_frames:\n",
    "                    frame_diff = int(frame) - 1\n",
    "                    valid_pairs.append((first_frame, z, frame_diff, frame, t))\n",
    "        \n",
    "        return valid_pairs\n",
    "\n",
    "    def _load_real_sample(self, patient_path: str, pair: Tuple, patient_id: str) -> Optional[DataSample]:\n",
    "        \"\"\"Load a real data sample, returning None if invalid\"\"\"\n",
    "        current, z, frame_diff, next_frame, t = pair\n",
    "        \n",
    "        file1 = os.path.join(patient_path, f\"{patient_id}_t{current:02d}_z{z:02d}.npy\")\n",
    "        file2 = os.path.join(patient_path, f\"{patient_id}_t{next_frame:02d}_z{z:02d}.npy\")\n",
    "        \n",
    "        if not (os.path.exists(file1) and os.path.exists(file2)):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            moving = np.load(file1).astype(np.float32)\n",
    "            fixed = np.load(file2).astype(np.float32)\n",
    "            \n",
    "            if len(moving.shape) == 2:\n",
    "                moving = moving[..., np.newaxis]\n",
    "            if len(fixed.shape) == 2:\n",
    "                fixed = fixed[..., np.newaxis]\n",
    "            \n",
    "            if moving.shape != (128, 128, 1) or fixed.shape != (128, 128, 1):\n",
    "                return None\n",
    "            \n",
    "            masks = None\n",
    "            if self.use_mask:\n",
    "                mask_folder = os.path.join(self.mask_root, patient_id)\n",
    "                mask_file1 = f\"{patient_id}_t{current:02d}_z{z:02d}_mask.npy\"\n",
    "                mask_file2 = f\"{patient_id}_t{next_frame:02d}_z{z:02d}_mask.npy\"\n",
    "                \n",
    "                mask_path1 = os.path.join(mask_folder, mask_file1)\n",
    "                mask_path2 = os.path.join(mask_folder, mask_file2)\n",
    "                \n",
    "                if not (os.path.exists(mask_path1) and os.path.exists(mask_path2)):\n",
    "                    return None\n",
    "                \n",
    "                mask_moving = np.load(mask_path1).astype(np.float32)\n",
    "                mask_fixed = np.load(mask_path2).astype(np.float32)\n",
    "                \n",
    "                if len(mask_moving.shape) == 2:\n",
    "                    mask_moving = mask_moving[..., np.newaxis]\n",
    "                if len(mask_fixed.shape) == 2:\n",
    "                    mask_fixed = mask_fixed[..., np.newaxis]\n",
    "                \n",
    "                if mask_moving.shape != (128, 128, 1) or mask_fixed.shape != (128, 128, 1):\n",
    "                    return None\n",
    "                \n",
    "                masks = (mask_moving, mask_fixed)\n",
    "            \n",
    "            sample = DataSample(\n",
    "                moving=moving,\n",
    "                fixed=fixed,\n",
    "                masks=masks,\n",
    "                patient_id=patient_id,\n",
    "                source=\"real\"\n",
    "            )\n",
    "            \n",
    "            if not self._validate_sample_shapes(sample) or not self._validate_sample_data(sample):\n",
    "                return None\n",
    "            \n",
    "            return sample\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error loading real sample {file1}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _load_simulated_sample(self, patient_path: str, pair: Tuple, patient_id: str) -> Optional[DataSample]:\n",
    "        \"\"\"Load a simulated data sample, returning None if invalid\"\"\"\n",
    "        current, z, frame_diff, next_frame, t = pair\n",
    "        \n",
    "        z_str, t_str = f\"{z:02d}\", f\"{t:02d}\"\n",
    "        base_name = patient_id.split('_z')[0] if '_z' in patient_id else patient_id\n",
    "        \n",
    "        file1 = os.path.join(patient_path, f\"{base_name}_t{t_str}_z{z_str}#{next_frame}_1.npy\")\n",
    "        file2 = os.path.join(patient_path, f\"{base_name}_t{t_str}_z{z_str}#{next_frame}.npy\")\n",
    "        \n",
    "        if not (os.path.exists(file1) and os.path.exists(file2)):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            moving = np.load(file1).astype(np.float32)\n",
    "            fixed = np.load(file2).astype(np.float32)\n",
    "            \n",
    "            if len(moving.shape) == 2:\n",
    "                moving = moving[..., np.newaxis]\n",
    "            if len(fixed.shape) == 2:\n",
    "                fixed = fixed[..., np.newaxis]\n",
    "            \n",
    "            if moving.shape != (128, 128, 1) or fixed.shape != (128, 128, 1):\n",
    "                return None\n",
    "            \n",
    "            masks = None\n",
    "            if self.use_mask:\n",
    "                mask_folder = os.path.join(self.simulated_mask_root, patient_id)\n",
    "                mask_file1 = f\"{base_name}_t{t_str}_z{z_str}#{next_frame}_1.npy\"\n",
    "                mask_file2 = f\"{base_name}_t{t_str}_z{z_str}#{next_frame}.npy\"\n",
    "                \n",
    "                mask_path1 = os.path.join(mask_folder, mask_file1)\n",
    "                mask_path2 = os.path.join(mask_folder, mask_file2)\n",
    "                \n",
    "                if not (os.path.exists(mask_path1) and os.path.exists(mask_path2)):\n",
    "                    return None\n",
    "                \n",
    "                mask_moving = np.load(mask_path1).astype(np.float32)\n",
    "                mask_fixed = np.load(mask_path2).astype(np.float32)\n",
    "                \n",
    "                if len(mask_moving.shape) == 2:\n",
    "                    mask_moving = mask_moving[..., np.newaxis]\n",
    "                if len(mask_fixed.shape) == 2:\n",
    "                    mask_fixed = mask_fixed[..., np.newaxis]\n",
    "                \n",
    "                if mask_moving.shape != (128, 128, 1) or mask_fixed.shape != (128, 128, 1):\n",
    "                    return None\n",
    "                \n",
    "                masks = (mask_moving, mask_fixed)\n",
    "            \n",
    "            sample = DataSample(\n",
    "                moving=moving,\n",
    "                fixed=fixed,\n",
    "                masks=masks,\n",
    "                patient_id=patient_id,\n",
    "                source=\"simulated\"\n",
    "            )\n",
    "            \n",
    "            if not self._validate_sample_shapes(sample) or not self._validate_sample_data(sample):\n",
    "                return None\n",
    "            \n",
    "            return sample\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error loading simulated sample {file1}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _validate_sample_shapes(self, sample: DataSample) -> bool:\n",
    "        \"\"\"Validate that a sample has correct shapes\"\"\"\n",
    "        if sample.moving.shape != (128, 128, 1):\n",
    "            return False\n",
    "        \n",
    "        if sample.fixed.shape != (128, 128, 1):\n",
    "            return False\n",
    "        \n",
    "        if self.use_mask and sample.masks:\n",
    "            mask_moving, mask_fixed = sample.masks\n",
    "            if mask_moving.shape != (128, 128, 1) or mask_fixed.shape != (128, 128, 1):\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _validate_sample_data(self, sample: DataSample) -> bool:\n",
    "        \"\"\"Validate sample data quality\"\"\"\n",
    "        # Check for NaN or infinite values\n",
    "        if np.isnan(sample.moving).any() or np.isinf(sample.moving).any():\n",
    "            return False\n",
    "        \n",
    "        if np.isnan(sample.fixed).any() or np.isinf(sample.fixed).any():\n",
    "            return False\n",
    "        \n",
    "        # Check data range (assuming normalized 0-1 or similar)\n",
    "        if sample.moving.min() < -10 or sample.moving.max() > 10:\n",
    "            return False\n",
    "        \n",
    "        if sample.fixed.min() < -10 or sample.fixed.max() > 10:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba636e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(output_dir: str = \"Training_data\", split: str = \"train\") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load preprocessed data from HDF5 files\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory containing the HDF5 files\n",
    "        split: Data split to load ('train', 'val', 'test')\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (moving, fixed, zero_phi) arrays\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_dir, f\"{split}_data.h5\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Preprocessed data file not found: {file_path}\")\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        moving = f['moving'][:]\n",
    "        fixed = f['fixed'][:]\n",
    "        zero_phi = f['zero_phi'][:]\n",
    "        \n",
    "        logging.info(f\"Loaded {split} data:\")\n",
    "        logging.info(f\"  Moving: {moving.shape}\")\n",
    "        logging.info(f\"  Fixed: {fixed.shape}\")\n",
    "        logging.info(f\"  Zero_phi: {zero_phi.shape}\")\n",
    "        logging.info(f\"  Use mask: {f.attrs.get('use_mask', 'unknown')}\")\n",
    "    \n",
    "    return moving, fixed, zero_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e9c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main function to run data preprocessing\"\"\"\n",
    "parser = argparse.ArgumentParser(description='Preprocess and save training data')\n",
    "parser.add_argument('--config', type=str, help='Path to configuration file')\n",
    "parser.add_argument('--output_dir', type=str, default='Training_data', help='Output directory for processed data')\n",
    "parser.add_argument('--chunk_size', type=int, default=1000, help='Chunk size for processing')\n",
    "parser.add_argument('--use_mask', action='store_true', help='Use masks in preprocessing')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Example configuration - replace with your actual paths\n",
    "real_data_paths={\n",
    "        'train': [train_data],\n",
    "        'val': [val_data],\n",
    "        'test': [test_data]\n",
    "    }\n",
    "\n",
    "simulated_data_paths={\n",
    "        'train': [train_simulated_data],\n",
    "        'val': [val_simulated_data],\n",
    "        'test': [test_simulated_data]\n",
    "    }\n",
    "\n",
    "mask_root_path=mask_data\n",
    "simulated_mask_root_path=mask_simulated_data\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = DataPreprocessor(\n",
    "    real_data_paths=real_data_paths,\n",
    "    simulated_data_paths=simulated_data_paths,\n",
    "    mask_root_path=mask_root_path,\n",
    "    simulated_mask_root_path=simulated_mask_root_path,\n",
    "    output_dir=args.output_dir,\n",
    "    use_mask=args.use_mask,\n",
    "    chunk_size=args.chunk_size,\n",
    "    seed=args.seed\n",
    ")\n",
    "\n",
    "# Process and save all splits\n",
    "preprocessor.process_and_save_all_splits()\n",
    "\n",
    "logging.info(\"Data preprocessing completed successfully!\")\n",
    "\n",
    "# Example of loading the data\n",
    "logging.info(\"Testing data loading...\")\n",
    "try:\n",
    "    train_moving, train_fixed, train_zero_phi = load_preprocessed_data(args.output_dir, 'train')\n",
    "    logging.info(\"Data loading test successful!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Data loading test failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
