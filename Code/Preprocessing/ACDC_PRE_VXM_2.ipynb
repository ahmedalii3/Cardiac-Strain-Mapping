{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Example: Accessing a file in your Google Drive\n",
        "ACDC_dataset_path = '/content/drive/My Drive/GP_Data_Folder/GP_Data-Sets/ACDC/database'\n",
        "print(f\"File path: {ACDC_dataset_path}\")\n",
        "\n",
        "# To get the current working directory:\n",
        "import os\n",
        "current_directory = os.getcwd()\n",
        "print(f\"\\nCurrent working directory: {current_directory}\")\n",
        "\n",
        "# To list files in a directory:\n",
        "import os\n",
        "directory_to_list = '/content/drive/My Drive'  # Replace with your desired directory\n",
        "try:\n",
        "  files = os.listdir(ACDC_dataset_path)\n",
        "  print(f\"\\nFiles and directories in {ACDC_dataset_path}:\")\n",
        "  for file in files:\n",
        "    print(file)\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: Directory not found at {ACDC_dataset_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4b-eAQdli6p",
        "outputId": "45d7d0ec-047d-46bd-ca88-34287315aee7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File path: /content/drive/My Drive/GP_Data_Folder/GP_Data-Sets/ACDC/database\n",
            "\n",
            "Current working directory: /content\n",
            "\n",
            "Files and directories in /content/drive/My Drive/GP_Data_Folder/GP_Data-Sets/ACDC/database:\n",
            "MANDATORY_CITATION.md\n",
            "LICENSE_TERMS.md\n",
            "STATE-OF-THE-ART-nnUNet-method.md\n",
            "training\n",
            "test_standardized\n",
            "train_standardized\n",
            "testing\n",
            "processed_training\n",
            "processed_testing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK nibabel pydicom gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7Hbbib6n29b",
        "outputId": "b1603036-3a35-4700-b14d-a96c2c1a2c70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.12.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MqnhRWrMlcaO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import logging\n",
        "import importlib.util\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import SimpleITK as sitk\n",
        "import scipy.interpolate as spi\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple, Optional, List\n",
        "import shutil\n",
        "\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    # BASE_PATH = Path(os.path.abspath(os.path.join(os.getcwd(), \"../../Data/ACDC/database\"))) # Locally\n",
        "    BASE_PATH = Path(ACDC_dataset_path) # Google Drive\n",
        "    TARGET_SPACING = (1.0, 1.0)  # (x, y)\n",
        "    TARGET_SHAPE = (512, 512)     # (height, width)\n",
        "    MAX_PATIENTS = None              # Set to None for all patients\n",
        "    SEED = 42                     # For reproducible sampling\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-LcBWjuglcaQ"
      },
      "outputs": [],
      "source": [
        "def process_4d_volume(nifti_path: Path, output_dir: Path) -> None:\n",
        "    \"\"\"Process a single 4D NIfTI file into standardized slices.\"\"\"\n",
        "    try:\n",
        "        img = nib.load(nifti_path)\n",
        "        data = img.get_fdata()  # Shape: (X, Y, Z, T)\n",
        "        patient_id = nifti_path.stem.split('_')[0]\n",
        "\n",
        "        for t in range(data.shape[-1]):  # Time frames\n",
        "            for z in range(data.shape[2]):  # Slices\n",
        "                process_slice(data[:, :, z, t], patient_id, t, z, output_dir)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed {nifti_path.name}: {str(e)}\")\n",
        "\n",
        "def process_slice(slice_data: np.ndarray, pid: str, t: int, z: int, output_dir: Path) -> None:\n",
        "    \"\"\"Process and save a single slice with localization.\"\"\"\n",
        "    try:\n",
        "        # Resample\n",
        "        slice_sitk = sitk.GetImageFromArray(slice_data.T)  # Handle axis order\n",
        "        resampled = resample_slice(slice_sitk, Config.TARGET_SPACING)\n",
        "\n",
        "        # Normalize\n",
        "        normalized = normalize_slice(sitk.GetArrayFromImage(resampled))\n",
        "\n",
        "        # Pad\n",
        "        padded = pad_to_shape(normalized, Config.TARGET_SHAPE)\n",
        "\n",
        "        # Save\n",
        "        save_path = output_dir / f\"{pid}_t{t:02d}_z{z:02d}.npy\"\n",
        "        np.save(save_path, padded.astype(np.float32))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed {pid} t{t} z{z}: {str(e)}\")\n",
        "\n",
        "def resample_slice(slice_sitk: sitk.Image, target_spacing: Tuple[float, float]) -> sitk.Image:\n",
        "    \"\"\"\n",
        "    Resample slice using your custom monotonic interpolation method.\n",
        "    Returns a SimpleITK image with correct metadata.\n",
        "    \"\"\"\n",
        "    # Get original metadata\n",
        "    original_spacing = slice_sitk.GetSpacing()\n",
        "    original_direction = slice_sitk.GetDirection()\n",
        "    original_origin = slice_sitk.GetOrigin()\n",
        "\n",
        "    # Calculate resize factor (X, Y only)\n",
        "    resize_factor = np.array([\n",
        "        original_spacing[0] / target_spacing[0],\n",
        "        original_spacing[1] / target_spacing[1]\n",
        "    ])\n",
        "\n",
        "    # Convert to numpy array (preserve axis order)\n",
        "    image_np = sitk.GetArrayFromImage(slice_sitk)  # Shape: (Z, Y, X) -> but Z=1 for 2D\n",
        "\n",
        "    # Remove singleton dimension if needed\n",
        "    if image_np.shape[0] == 1:\n",
        "        image_np = image_np[0]  # Shape: (Y, X)\n",
        "\n",
        "    # Apply your custom interpolation\n",
        "    resampled_np = monotonic_zoom_interpolate(\n",
        "        image_np,\n",
        "        resize_factor[::-1]  # Reverse for (Y, X) axes\n",
        "    )\n",
        "\n",
        "    # Create new SimpleITK image\n",
        "    resampled_sitk = sitk.GetImageFromArray(resampled_np)\n",
        "\n",
        "    # Set metadata correctly\n",
        "    resampled_sitk.SetSpacing((target_spacing[0], target_spacing[1]))\n",
        "    resampled_sitk.SetDirection(original_direction)\n",
        "    resampled_sitk.SetOrigin(original_origin)\n",
        "\n",
        "    return resampled_sitk\n",
        "\n",
        "def normalize_slice(slice_np: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Robust normalization using percentile clipping.\"\"\"\n",
        "    p1, p99 = np.percentile(slice_np, [1, 99])\n",
        "    return np.clip((slice_np - p1) / (p99 - p1), 0, 1)\n",
        "\n",
        "def pad_to_shape(slice_np: np.ndarray, target_shape: Tuple[int, int]) -> np.ndarray:\n",
        "    \"\"\"Symmetrically pad slice to target shape.\"\"\"\n",
        "    pads = [(max(0, (ts - s) // 2), max(0, ts - s - (ts - s) // 2))\n",
        "            for s, ts in zip(slice_np.shape, target_shape)]\n",
        "    return np.pad(slice_np, pads, mode='constant')\n",
        "\n",
        "def monotonic_zoom_interpolate(image_np, resize_factor):\n",
        "    \"\"\"\n",
        "    Apply monotonic zoom interpolation to a given image.\n",
        "    \"\"\"\n",
        "    result = image_np.copy()\n",
        "\n",
        "    for axis, factor in enumerate(resize_factor[::-1]):\n",
        "        # Create a new array for the interpolated values\n",
        "        new_length = int(result.shape[axis] * factor)\n",
        "        x_old = np.arange(result.shape[axis])\n",
        "        x_new = np.linspace(0, result.shape[axis] - 1, new_length)\n",
        "\n",
        "        # Perform monotonic interpolation\n",
        "        pchip_interp = spi.PchipInterpolator(x_old, result.take(indices=x_old, axis=axis), axis=axis)\n",
        "        result = pchip_interp(x_new)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4wNIAE4hlcaR"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(dataset_type: str = \"training\", output_dir_name: Optional[str] = None) -> None:\n",
        "    \"\"\"\n",
        "    Main pipeline execution function.\n",
        "\n",
        "    Args:\n",
        "        dataset_type: Type of dataset to process (e.g., \"training\", \"testing\").\n",
        "        output_dir_name: Custom name for the output directory. If None, defaults to \"processed_{dataset_type}\".\n",
        "    \"\"\"\n",
        "    input_dir = Config.BASE_PATH / dataset_type\n",
        "\n",
        "    # Set output directory name\n",
        "    if output_dir_name is None:\n",
        "        output_dir_name = f\"processed_{dataset_type}\"\n",
        "    output_dir = Config.BASE_PATH / output_dir_name\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    nifti_files = list(input_dir.glob(\"**/*_4d.nii.gz\"))\n",
        "    if Config.MAX_PATIENTS:\n",
        "        nifti_files = nifti_files[:Config.MAX_PATIENTS]\n",
        "\n",
        "    logger.info(f\"Processing {len(nifti_files)} patients...\")\n",
        "    for nifti_path in tqdm(nifti_files, desc=\"Patients\"):\n",
        "        process_4d_volume(nifti_path, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4d92-u-GlcaR"
      },
      "outputs": [],
      "source": [
        "def validate_processing(output_dir: Path, num_samples: int = 5, dataset_type: str = \"training\") -> None:\n",
        "    \"\"\"Validate processed data with multiple checks.\"\"\"\n",
        "    # Check file consistency\n",
        "    npy_files = list(output_dir.glob(\"*.npy\"))\n",
        "    assert len(npy_files) > 0, \"No processed files found!\"\n",
        "\n",
        "    # Check array properties\n",
        "    sample = np.load(npy_files[0])\n",
        "    assert sample.shape == Config.TARGET_SHAPE, f\"Shape mismatch: {sample.shape}\"\n",
        "    assert sample.dtype == np.float32, f\"Dtype mismatch: {sample.dtype}\"\n",
        "\n",
        "    # Value range check\n",
        "    min_val, max_val = sample.min(), sample.max()\n",
        "    assert 0 <= min_val <= max_val <= 1, f\"Value range error: [{min_val}, {max_val}]\"\n",
        "\n",
        "    logger.info(\"Basic validation passed!\")\n",
        "\n",
        "    # New: Intensity distribution check\n",
        "    sample_values = sample.flatten()\n",
        "    hist, bins = np.histogram(sample_values, bins=100)\n",
        "    assert np.percentile(sample_values, 99) > 0.1, \"Suspicious intensity distribution\"\n",
        "\n",
        "    logger.info(\"Enhanced validation passed!\")\n",
        "\n",
        "    # Visualize samples\n",
        "    visualize_samples(output_dir, num_samples, dataset_type)\n",
        "    logger.info(\"Visualization check passed!\")\n",
        "\n",
        "def visualize_samples(output_dir: Path, num_samples: int = 5, dataset_type: str = \"training\") -> None:\n",
        "    \"\"\"Visualize random samples with before/after comparison and save as PNG.\"\"\"\n",
        "    # Create a folder to save the visualizations\n",
        "    print(output_dir)\n",
        "    visualization_dir = output_dir / \"visualizations\"\n",
        "    if not visualization_dir.exists():\n",
        "        visualization_dir.mkdir(parents=True, exist_ok=True)\n",
        "        logger.info(f\"Created visualization directory: {visualization_dir}\")\n",
        "    else:\n",
        "        logger.info(f\"Visualization directory already exists: {visualization_dir}\")\n",
        "\n",
        "    np.random.seed(Config.SEED)\n",
        "    sample_files = np.random.choice(list(output_dir.glob(\"*.npy\")), num_samples)\n",
        "\n",
        "    for sf in sample_files:\n",
        "        # Parse filename to get patient ID, time frame, and slice index\n",
        "        parts = sf.stem.split('_')\n",
        "        patient_id = parts[0]\n",
        "        time_frame = int(parts[1][1:])  # Extract number after 't'\n",
        "        slice_index = int(parts[2][1:])  # Extract number after 'z'\n",
        "\n",
        "        # Locate original 4D NIfTI file\n",
        "        original_path = find_original_image(sf, dataset_type)\n",
        "        if not original_path:\n",
        "            logger.warning(f\"Original image not found for {sf.name}\")\n",
        "            continue\n",
        "\n",
        "        # Load original 4D NIfTI file\n",
        "        orig_img = sitk.GetArrayFromImage(sitk.ReadImage(str(original_path)))  # Shape: (T, Z, Y, X)\n",
        "        orig_img = orig_img.transpose()\n",
        "\n",
        "        # Ensure the dimensions are correct\n",
        "        if orig_img.ndim != 4:\n",
        "            logger.error(f\"Original image {original_path.name} is not 4D. Shape: {orig_img.shape}\")\n",
        "            continue\n",
        "\n",
        "        # Extract the corresponding slice and time frame\n",
        "        try:\n",
        "            # Correctly extract the slice: (X, Y, Z, T) -> (X, Y) for the given Z and T\n",
        "            orig_slice = orig_img[:, :, slice_index, time_frame].T  # Transpose for correct orientation\n",
        "        except IndexError:\n",
        "            logger.error(f\"Invalid slice or time frame for {sf.name}: slice={slice_index}, frame={time_frame}\")\n",
        "            continue\n",
        "\n",
        "        # Load processed numpy file\n",
        "        processed = np.load(sf)  # Shape: (H, W)\n",
        "\n",
        "        # Plot original and processed slices\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "        # Original slice\n",
        "        ax1.imshow(orig_slice, cmap='gray')\n",
        "        ax1.set_title(f\"Original\\n{original_path.name}\\nSlice {slice_index}, Frame {time_frame}\")\n",
        "\n",
        "        # Processed slice\n",
        "        ax2.imshow(processed, cmap='gray')\n",
        "        ax2.set_title(f\"Processed\\n{sf.name}\")\n",
        "\n",
        "        # Save the figure as a PNG file\n",
        "        output_file = visualization_dir / f\"{sf.stem}_comparison.png\"\n",
        "        fig.savefig(output_file, bbox_inches=\"tight\", dpi=300)\n",
        "        plt.close(fig)  # Close the figure to free memory\n",
        "        logger.info(f\"Saved visualization: {output_file}\")\n",
        "\n",
        "def find_original_image(processed_path: Path, dataset_type: str = \"training\") -> Optional[Path]:\n",
        "    \"\"\"\n",
        "    Locate original NIfTI file from processed numpy path.\n",
        "\n",
        "    Args:\n",
        "        processed_path: Path to the processed numpy file.\n",
        "        dataset_type: Type of dataset to search in (e.g., \"training\", \"testing\").\n",
        "\n",
        "    Returns:\n",
        "        Path to the original NIfTI file, or None if not found.\n",
        "    \"\"\"\n",
        "    parts = processed_path.stem.split('_')\n",
        "    patient_id = parts[0]\n",
        "\n",
        "    # Search in the correct dataset folder\n",
        "    patient_folder = Config.BASE_PATH / dataset_type / patient_id\n",
        "    return next(patient_folder.glob(\"*4d.nii.gz\"), None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQusFWlClcaR",
        "outputId": "bd65edfd-5c77-4e76-bf93-21c9852af166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Patients:  84%|████████▍ | 84/100 [17:15<02:23,  8.97s/it]<ipython-input-4-e3dd7c57d4b5>:77: RuntimeWarning: invalid value encountered in divide\n",
            "  return np.clip((slice_np - p1) / (p99 - p1), 0, 1)\n",
            "Patients: 100%|██████████| 100/100 [20:26<00:00, 12.27s/it]\n",
            "Patients: 100%|██████████| 50/50 [10:26<00:00, 12.52s/it]\n"
          ]
        }
      ],
      "source": [
        "# Process training data\n",
        "run_pipeline(\"training\", \"processed_training\")\n",
        "\n",
        "# Process testing data (optional)\n",
        "run_pipeline(\"testing\", \"processed_testing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wgtejDAylcaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b57667-6963-4af7-f66c-ac54227083f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/GP_Data_Folder/GP_Data-Sets/ACDC/database/processed_testing\n"
          ]
        }
      ],
      "source": [
        "# output_dir = Config.BASE_PATH / \"processed_training\"\n",
        "# validate_processing(output_dir, num_samples=1000, dataset_type=\"training\")\n",
        "output_dir = Config.BASE_PATH / \"processed_testing\"\n",
        "validate_processing(output_dir, num_samples=500, dataset_type=\"testing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DpdDyaFlcaS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}